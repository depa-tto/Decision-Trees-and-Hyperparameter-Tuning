{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "import random # type: ignore\n",
    "from pprint import pprint # type: ignore\n",
    "import plotly.express as px # type: ignore\n",
    "from tabulate import tabulate # type: ignore\n",
    "from sklearn.metrics import confusion_matrix # type: ignore\n",
    "from sklearn.metrics import accuracy_score # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://raw.githubusercontent.com/depa-tto/Machine-Learning-Module-Antonio-De-Patto/main/mashroom_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(path, sep = ';')\n",
    "\n",
    "df = df.rename(columns={\"class\": \"label\"})\n",
    "temp_cols = df.columns.tolist()\n",
    "new_cols = temp_cols[1:] + temp_cols[0:1]\n",
    "df = df[new_cols]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = df.isnull().sum()\n",
    "\n",
    "table = [[col, na_counts[col]] for col in na_counts.index]\n",
    "col_names = [\"Features\", \"NA\"]\n",
    "\n",
    "print(tabulate(table, headers=col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['gill-spacing','stem-root', 'stem-surface', 'veil-type', 'veil-color', 'spore-print-color'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cap-shape'] = df['cap-shape'].map({'b':'bell', 'c':'conical', 'x':'convex', 'f':'flat', 's':'sunken', 'p':'spherical', 'o':'others'})\n",
    "df['cap-surface'] = df['cap-surface'].map({'i':'fibrous', 'g':'grooves', 'y':'scaly', 's':'smooth', 'h':'shiny', 'l':'leathery', 'k':'silky', 't':'sticky', 'w':'wrinkled', 'e':'fleshy', 'd': 'dry'})\n",
    "df['cap-color'] = df['cap-color'].map({'n':'brown', 'b':'buff', 'g':'gray', 'r':'green', 'p':'pink', 'u':'purple', 'e':'red', 'w':'white', 'y':'yellow', 'l':'blue', 'o':'orange', 'k':'black'})\n",
    "df['does-bruise-or-bleed'] = df['does-bruise-or-bleed'].map({'t':'bruises-bleedin', 'f':'not-bruises-bleedin'})\n",
    "df['gill-attachment'] = df['gill-attachment'].map({'a':'bell', 'x':'conical', 'd':'convex', 'e':'flat', 's':'sunken', 'p':'spherical', 'f':'none', '?':'none'})\n",
    "df['gill-color'] = df['gill-color'].map({'n':'brown', 'b':'buff', 'g':'gray', 'r':'green', 'p':'pink', 'u':'purple', 'e':'red', 'w':'white', 'y':'yellow', 'l':'blue', 'o':'orange', 'k':'black', 'f':'none'})\n",
    "df['stem-color'] = df['stem-color'].map({'n':'brown', 'b':'buff', 'g':'gray', 'r':'green', 'p':'pink', 'u':'purple', 'e':'red', 'w':'white', 'y':'yellow', 'l':'blue', 'o':'orange', 'k':'black', 'f':'none'})\n",
    "df['has-ring'] = df['has-ring'].map({'t':'ring', 'f':'none'})\n",
    "df['ring-type'] = df['ring-type'].map({'c':'cobwebby', 'e':'evanescent', 'r':'flaring', 'g':'grooved', 'l':'large', 'p':'pendant', 's':'sheathing', 'z':'zone', 'y':'scaly', 'm':'movable', 'f':'none', '?':'none'})\n",
    "df['habitat'] = df['habitat'].map({'g':'grasses', 'l':'leaves', 'm':'meadows', 'p':'paths', 'h':'heaths', 'u':'urban', 'w':'waste', 'd':'woods'})\n",
    "df['season'] = df['season'].map({'s':'spring', 'u':'summer', 'a':'autumn', 'w':'winter'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = px.pie(df, names='label', color='label', color_discrete_sequence=['#008066','#B2D966'], title='Data Distribution')\n",
    "\n",
    "dis.update_traces(textfont_size=18)  \n",
    "dis.update_layout(width=700, height=500, plot_bgcolor='white', paper_bgcolor='white',\n",
    "                  legend=dict(x=0.8, y=1, traceorder='normal', orientation='v', title_font=dict(size=16), font=dict(size=16)))\n",
    "\n",
    "dis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['label'].value_counts()\n",
    "\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select_dtypes(include=np.number).columns:\n",
    "    \n",
    "    fig = px.box(data_frame=df, x='label', color='label', y=column, color_discrete_sequence=['#B2D966', '#008066'], orientation='v')\n",
    "    fig.update_layout(\n",
    "        width=600,   \n",
    "        height=400,  \n",
    "        plot_bgcolor='white', \n",
    "        paper_bgcolor='white',\n",
    "        title=f'Box plot of {column}', \n",
    "        xaxis_title='Label',\n",
    "        yaxis_title=column,\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cormat = df.select_dtypes(include=np.number).corr()\n",
    "round(cormat,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cormat, annot=True, cmap='summer', cbar_kws={'shrink': .8}, linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df, hue = 'label', palette=['#B2D966', '#008066'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'label' \n",
    "\n",
    "for column in df.drop(columns=[target_column]).select_dtypes(exclude=[np.number]).columns:\n",
    "\n",
    "    crosstab = pd.crosstab(df[column], df[target_column]).reset_index() \n",
    "\n",
    "    crosstab['total'] = crosstab['e'] + crosstab['p'] \n",
    "    crosstab = crosstab.sort_values(by='total', ascending=False)\n",
    "\n",
    "    crosstab_melted = crosstab.melt(id_vars=[column], value_vars=['e', 'p'], \n",
    "                                    var_name=target_column, value_name='Count')\n",
    "    fig = px.bar(crosstab_melted, \n",
    "                 x=column, \n",
    "                 y='Count', \n",
    "                 color=target_column,\n",
    "                 labels={column: column, 'Count': 'Count', target_column: 'Type'},\n",
    "                 title=f'Frequencies of Edible and Poisonous mushroom for {column}',\n",
    "                 color_discrete_map={'e': '#008066', 'p': '#B2D966'})\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=550,\n",
    "        height=450,\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        barmode='stack',  \n",
    "        xaxis_title=column,\n",
    "        yaxis_title='Count',\n",
    "        legend=dict(\n",
    "            x=0.9,  \n",
    "            y=0.9,     \n",
    "            title='Type',\n",
    "            traceorder='normal',\n",
    "            orientation='v'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(categoryorder='total descending')\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cap-diameter'] = round(df['cap-diameter'], 3)\n",
    "df['stem-width'] = round(df['stem-width'], 3)\n",
    "df['stem-height'] = round(df['stem-height'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cap-diameter'] = pd.to_numeric(df['cap-diameter'], errors='coerce')\n",
    "df['stem-width'] = pd.to_numeric(df['stem-width'], errors='coerce')\n",
    "df['stem-height'] = pd.to_numeric(df['stem-height'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, test_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Splits the given dataset into training and testing subsets.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: the input dataset to be split.\n",
    "    - test_size: specifies the size of the test set. Can be either:\n",
    "        - An integer: number of rows for the test set.\n",
    "        - A float: proportion of the total number of rows to be used as the test set.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple (train_df, test_df) where:\n",
    "        - train_df: dataset containing the training data.\n",
    "        - test_df: dataset containing the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(test_size, float): # if the test size is a number or if is a proportion(float)\n",
    "        test_size = round(test_size * len(df)) # we have to compute the number of rows this proportion represents\n",
    "\n",
    "    indices = df.index.tolist() # list of row indices from the dataset\n",
    "    test_indices = random.sample(population = indices, k = test_size) # we want to pick at random a certain number of these indices from this list\n",
    "\n",
    "    test_df = df.loc[test_indices] # we create the test daframe by just indexing the rows with test indices\n",
    "    train_df = df.drop(test_indices) # and the training set in created by dropping rows with the test indices\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_df, test_df = split_train_test(df, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df.values # we transfrom from a pandas df to a numpy 2d array to make everything faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(column, '-', len(df[column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_features(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Categorizes the features in the dataset into either 'categorical' or 'continuous'.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The input dataset containing the features to be classified \n",
    "    \n",
    "    Returns:\n",
    "    - A list `feature_types` where each element corresponds to the type of a feature in the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_types = [] # we initialize an empty list to store the type of each feature\n",
    "    n_unique_values_treshold = 15 # threshold for the number of unique values to determine if a feature is categorical\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        if feature != \"label\":\n",
    "            unique_values = df[feature].unique() # we get the unique values in the feature column\n",
    "            example_value = unique_values[0] # we initially pick the first value from the unique values as a sample\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_TYPES = categorize_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_TYPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is a node pure ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_purity(data):   \n",
    "\n",
    "    \"\"\"\n",
    "    Evaluates whether the data is pure, meaning all examples in the data belong to the same class.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "    \n",
    "    Returns:\n",
    "    - A boolean value:\n",
    "      - True: if all the examples in the data belong to the same class (i.e., the data is pure).\n",
    "      - False: if the examples belong to different classes (i.e., the data is impure).\n",
    "    \"\"\"\n",
    "\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column) # how many distinct classes are in this array ? we use the numpy function 'unique'\n",
    "\n",
    "    if len(unique_classes) == 1: \n",
    "        return True # if there's only one unique class, the data is pure\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = data[:, -1]\n",
    "unique_classes = np.unique(label_column) \n",
    "\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_purity(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_purity(train_df[train_df.label == 'e'].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_purity(train_df[train_df['cap-diameter'] > 18].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_majority_class(data):\n",
    "\n",
    "    \"\"\"\n",
    "    Determines the majority class in the given data, so in this case we want to classify if a data is poisoned or not\n",
    "    \n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "    \n",
    "    Returns:\n",
    "    - classification: the class that occurs most frequently in the label column.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "\n",
    "    # we are going to find the unique classes and their respective counts in the label column\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts = True) \n",
    "\n",
    "    # we need to know the index of the largest value of the 'counts_unique_classes' array to see which is the class that appears most often\n",
    "    # so we want to identify the index of the class with the highest count (most frequent class)\n",
    "    index = counts_unique_classes.argmax() \n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = data[:, -1]\n",
    "unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes, counts_unique_classes # in this case we can see that the p class is the one that appears most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = counts_unique_classes.argmax() \n",
    "classification = unique_classes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification # so p is the label that appears most often, and it is indexed 1, so it is in position 1 in the 'counts_unique_classes' array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_majority_class(train_df[train_df['cap-diameter'] < 10].values) # so considering 'cap-diameter' lower than 10 the p category is the one that appear most "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_possible_splits(data): # data which is a 2d numpy array\n",
    "\n",
    "    \"\"\"\n",
    "    Identifies all possible splits for each feature in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "\n",
    "    Returns:\n",
    "    - potential_splits: a dictionary where the keys are column indices (features), and the values are arrays of possible split points.\n",
    "    \"\"\"\n",
    "    \n",
    "    potential_splits = {} # dictionary, that has as keys the indeces of the columns and as values the list that contains all the potential splits\n",
    "    _, n_columns = data.shape # tuple that return the number of rows and the number of columns that we have in the dataframe, we only take the number of columns\n",
    "    for column_index in range(n_columns - 1): # excluding the last column which is the label\n",
    "        values = data[:, column_index] # values from the current feature column\n",
    "        unique_values = np.unique(values) # all unique values in this column\n",
    "        \n",
    "        potential_splits[column_index] = unique_values # for every columns we are going to create an entry in our potential split dictionary, so we are going to append our potential split\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_possible_splits(data) # the keys are the indecis of the columns and the values are the rows that contain numbers for the potential split, that are the unique values for every columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_split = find_possible_splits(data)\n",
    "\n",
    "sns.lmplot(data = train_df, x = 'cap-diameter', y = 'stem-width', hue = 'label', fit_reg = False)\n",
    "\n",
    "# plt.vlines(x = potential_split[0], ymin = 0, ymax = 100)\n",
    "# plt.hlines(y = potential_split[8], xmin = 0, xmax = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = train_df, x = 'cap-diameter', y = 'stem-height', hue = 'label', fit_reg = False)\n",
    "\n",
    "# plt.vlines(x = potential_split[0], ymin = 0, ymax = 100)\n",
    "# plt.hlines(y = potential_split[8], xmin = 0, xmax = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = train_df, x = 'cap-diameter', y = 'stem-width', hue = 'label', fit_reg = False)\n",
    "\n",
    "# plt.vlines(x = potential_split[0], ymin = 0, ymax = 100)\n",
    "# plt.hlines(y = potential_split[8], xmin = 0, xmax = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_column, split_value): \n",
    "\n",
    "    \"\"\"\n",
    "    Splits the data into two subsets based on a specified feature and split value.\n",
    "\n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "    - split_column: the index of the column (feature) on which to split the data.\n",
    "    - split_value: the value in the feature column to use as the threshold for splitting.\n",
    "\n",
    "    Returns:\n",
    "    - data_below: subset of data where the feature values are less than or equal to the split value (for continuous features) or equal to the split value (for categorical features).\n",
    "    - data_above: subset of data where the feature values are greater than the split value (for continuous features) or not equal to the split value (for categorical features).\n",
    "    \"\"\"\n",
    "    \n",
    "    split_column_values = data[:, split_column]  # we extract all the values from the specified column\n",
    "\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    if type_of_feature == \"continuous\": # if the feature is continuous, split based on the threshold value\n",
    "        data_below = data[split_column_values <= split_value] # data below the split value\n",
    "        data_above = data[split_column_values >  split_value] # data above the split value\n",
    "    \n",
    "    # feature is categorical   \n",
    "    else: # if the feature is categorical, split based on matching the value\n",
    "        data_below = data[split_column_values == split_value]\n",
    "        data_above = data[split_column_values != split_value]\n",
    "    \n",
    "    return data_below, data_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example continuos variable\n",
    "\n",
    "split_column = 0\n",
    "split_value = 5\n",
    "\n",
    "split_column_values = data[:, split_column]\n",
    "\n",
    "split_column_values <= split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example categorical variables \n",
    "\n",
    "split_column = 1\n",
    "split_value == 'b'\n",
    "\n",
    "split_column_values = data[:, split_column]\n",
    "\n",
    "split_column_values == split_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_entropy(data): \n",
    "\n",
    "    \"\"\"\n",
    "    Computes the scaled entropy of a dataset\n",
    "\n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "\n",
    "    Returns:\n",
    "    - entropy: the scaled entropy value for the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts = True) # unique classes and their counts in the label column\n",
    "\n",
    "    probabilities = counts / counts.sum() # probability of each class\n",
    "    entropy = sum((- probabilities/2) * np.log2(probabilities + 1e-10) - (1 - probabilities)/2 * np.log2(1 + 1e-10 - probabilities))\n",
    "     \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counts = np.unique(label_column, return_counts=True)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = counts / counts.sum()\n",
    "\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_entropy(data) # near to one, so there is a high confunsion between the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_entropy(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_entropy(data_below, data_above): # we will compute the entropy belonging to the below data and to the above data\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the weighted average of the entropy for two subsets of data (data_below and data_above).\n",
    "    This is used to evaluate the quality of a split in a decision tree.\n",
    "\n",
    "    Parameters:\n",
    "    - data_below: a 2D numpy array representing the subset of data that falls below a certain split value.\n",
    "    - data_above: a 2D numpy array representing the subset of data that falls above a certain split value.\n",
    "\n",
    "    Returns:\n",
    "    - overall_entropy: the weighted entropy of the combined subsets.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data_below) + len(data_above) # total number of data points in both subsets\n",
    "    p_data_below = len(data_below) / n # proportion of data points in the 'data_below' subset\n",
    "    p_data_above = len(data_above) / n # proportion of data points in the 'data_above' subset\n",
    "\n",
    "    # we compute the overall weighted entropy by taking the weighted sum of the entropies of both subsets\n",
    "    overall_entropy =  (p_data_below * scaled_entropy(data_below) \n",
    "                      + p_data_above * scaled_entropy(data_above))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity (data):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the Gini impurity of a dataset\n",
    "\n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "\n",
    "    Returns:\n",
    "    - gini: the Gini impurity of the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    gini = sum(2*probabilities * (1 - probabilities))\n",
    "     \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_impurity (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_gini(data_below, data_above):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the weighted Gini impurity for a given split of data.\n",
    "\n",
    "    Parameters:\n",
    "    - data_below: a subset of the data after applying a split, containing instances that fall below the split value.\n",
    "    - data_above: a subset of the data after applying a split, containing instances that fall above the split value.\n",
    "\n",
    "    Returns:\n",
    "    - overall_gini: the weighted Gini impurity of the split.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_gini =  (p_data_below * gini_impurity (data_below) \n",
    "                    + p_data_above * gini_impurity (data_above))\n",
    "    \n",
    "    return overall_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bhattacharyya coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya_coefficient (data):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the Bhattacharyya Coefficient of a dataset\n",
    "\n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "\n",
    "    Returns:\n",
    "    - bhatt_coeff: the Bhattacharyya Coefficient of the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    bhatt_coeff = sum(np.sqrt(probabilities * (1 - probabilities)))\n",
    "\n",
    "    return bhatt_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhattacharyya_coefficient (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_bhattacharyya_coefficient (data_below, data_above):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the weighted Bhattacharyya coefficient for a given split of data.\n",
    "\n",
    "    Parameters:\n",
    "    - data_below: a subset of the data after applying a split, containing instances that fall below the split value.\n",
    "    - data_above: a subset of the data after applying a split, containing instances that fall above the split value.\n",
    "\n",
    "    Returns:\n",
    "    - overall_bhatt_coeff: the weighted Bhattacharyya coefficient of the split.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_bhatt_coeff = (p_data_below * bhattacharyya_coefficient (data_below) \n",
    "                      + p_data_above * bhattacharyya_coefficient (data_above))\n",
    "    \n",
    "    return overall_bhatt_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine best split according to the Scaled Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would like to look at all the potential split and determine the one split that result in the lowest overall entropy \n",
    "\n",
    "def determine_best_split_scaled_entropy(data, potential_splits):\n",
    "\n",
    "    \"\"\"\n",
    "    Determines the best split for the dataset based on the scaled entropy criterion.\n",
    "    This function should look at all the potential split and then determine the one split that result in the lowest overall entropy\n",
    "\n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "    - potential_splits: a dictionary where keys are column indices and values are lists of potential split values for those columns.\n",
    "\n",
    "    Returns:\n",
    "    - best_split_column: the index of the column where the best split occurs.\n",
    "    - best_split_value: the value of the feature where the best split occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_entropy = 9999  # we initialize the overall entropy with a very high value\n",
    "    for column_index in potential_splits: # so this will loop over the keys that are the column indices\n",
    "        for value in potential_splits[column_index]: # this will loop over all the elements of the dictionary\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value) # split the data based on the current feature and split value\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above) # calculate the overall entropy for the current split\n",
    "\n",
    "            if current_overall_entropy <= overall_entropy: # is the current overall entropy smaller or equal than the initial overall entropy ? it can be equal because there could be more split that gives back the same cut in the entropy\n",
    "                overall_entropy = current_overall_entropy # if yes we are going to update the overall entropy and we are going to save this values in the 'best_split_column' and 'best_split_value' values\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine best split according to the Gini Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split_gini(data, potential_splits):\n",
    "\n",
    "    \"\"\"\n",
    "    Determines the best split for the dataset based on the Gini impurity criterion.\n",
    "    This function should look at all the potential split and then determine the one split that result in the lowest overall gini impurity\n",
    "\n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "    - potential_splits: a dictionary where keys are column indices and values are lists of potential split values for those columns.\n",
    "\n",
    "    Returns:\n",
    "    - best_split_column: the index of the column where the best split occurs.\n",
    "    - best_split_value: the value of the feature where the best split occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_gini = 9999\n",
    "    for column_index in potential_splits: \n",
    "        for value in potential_splits[column_index]: \n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_overall_gini = calculate_overall_gini(data_below, data_above)\n",
    "\n",
    "            if current_overall_gini <= overall_gini:\n",
    "                overall_gini = current_overall_gini \n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine best split according to the Bhattacharyya coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split_bhattacharyya_coefficient(data, potential_splits):\n",
    "\n",
    "    \"\"\"\n",
    "    Determines the best split for the dataset based on the Bhattacharyya coefficient criterion.\n",
    "    This function should look at all the potential split and then determine the one split that result in the lowest overall Bhattacharyya coefficient\n",
    "\n",
    "    Parameters:\n",
    "    - data: a 2D numpy array of the training set where each row is an example and the last column contains the labels.\n",
    "    - potential_splits: a dictionary where keys are column indices and values are lists of potential split values for those columns.\n",
    "\n",
    "    Returns:\n",
    "    - best_split_column: the index of the column where the best split occurs.\n",
    "    - best_split_value: the value of the feature where the best split occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_3 = 9999\n",
    "    for column_index in potential_splits: \n",
    "        for value in potential_splits[column_index]: \n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_overall_3 = calculate_overall_bhattacharyya_coefficient (data_below, data_above)\n",
    "\n",
    "            if current_overall_3 <= overall_3:\n",
    "                overall_3 = current_overall_3 \n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_method(data, potential_splits, criterion='gini'):\n",
    "    if criterion == 'scaled_entropy':\n",
    "        return determine_best_split_scaled_entropy(data, potential_splits)\n",
    "    elif criterion == 'gini':\n",
    "        return determine_best_split_gini(data, potential_splits)\n",
    "    elif criterion == 'bhatt_coeff':\n",
    "        return determine_best_split_bhattacharyya_coefficient(data, potential_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(df, counter = 0, min_samples = 200, max_depth = 5, criterion = 'scaled_entropy'):\n",
    "\n",
    "    \"\"\"\n",
    "    Constructs a decision tree for the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df: the input dataframe where rows are samples and columns include features and the label.\n",
    "    - counter: the current depth of the tree (used for recursion).\n",
    "    - min_samples: minimum number of samples required to split an internal node.\n",
    "    - max_depth: maximum depth of the tree.\n",
    "    - criterion: the criterion used to choose the best split ('scaled_entropy', 'gini', 'bhatt_coeff').\n",
    "\n",
    "    Returns:\n",
    "    - A decision tree \n",
    "    \"\"\"\n",
    "    \n",
    "    # data preparations for the first function call. we initialize global variables for column headers and feature types\n",
    "    if counter == 0: # so in the first call of the function we give a general information about the data since all the helper function works for a 2d numpy array \n",
    "        global COLUMN_HEADERS, FEATURE_TYPES # we specify these variables as globals\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = categorize_features(df)\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df # for recursive calls, use the provided data          \n",
    "    \n",
    "    \n",
    "    # base cases, where the stopping conditions are presented\n",
    "    # evaluate_purity gives back a boolean array that can be directly be classifies\n",
    "    # also we classify a data if there are not 'min_samples' datapoints, even though it could not be pure yet\n",
    "    # so if in a particular node the number of samples becomes less than the minimum samples then we will not split that node any further and it will be a leaf node\n",
    "    # if the depth of the tree reach the maximum depth we will not split the nodes further\n",
    "    if (evaluate_purity(data)) or (len(data) < min_samples) or (counter == max_depth): \n",
    "        classification = determine_majority_class(data)\n",
    "        \n",
    "        return classification\n",
    "\n",
    "    \n",
    "    # recursive part:split the data and continue building the tree\n",
    "    else:    \n",
    "        counter += 1\n",
    "\n",
    "        potential_splits = find_possible_splits(data) # we determine possible splits for each feature\n",
    "        split_column, split_value = split_method(data, potential_splits, criterion=criterion) # the we select the best split based on the chosen criterion\n",
    "        data_below, data_above = split_data(data, split_column, split_value) # and finally we split the data into two subsets based on the best split\n",
    "        \n",
    "        # check for case where a split results in empty subsets\n",
    "        if len(data_below) == 0 or len(data_above) == 0: \n",
    "            classification = determine_majority_class(data)\n",
    "            return classification\n",
    "        \n",
    "        # determine question for the decision tree node\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            question = \"{} <= {}\".format(feature_name, split_value)\n",
    "            \n",
    "        # feature is categorical\n",
    "        else:\n",
    "            question = \"{} = {}\".format(feature_name, split_value)\n",
    "        \n",
    "        # creation of a new sub-tree with the formulated question\n",
    "        sub_tree = {question: []} # in that empty list we want to append the yes or no answer \n",
    "        \n",
    "        # recursively build the subtrees for the 'yes' and 'no' branches of the current split\n",
    "        yes_answer = decision_tree(data_below, counter, min_samples, max_depth, criterion)\n",
    "        no_answer = decision_tree(data_above, counter, min_samples, max_depth, criterion)\n",
    "        \n",
    "        # if the answers are the same, then there is no point in asking the question and we can use that answer directly\n",
    "        # this could happen when the data is classified even though it is not pure yet (min_samples or max_depth base case).\n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_example(example, tree):\n",
    "\n",
    "    \"\"\"\n",
    "    Predicts the class label for a given example using the provided decision tree.\n",
    "\n",
    "    Parameters:\n",
    "    - example: a dictionary representing a single data instance with feature names as keys.\n",
    "    - tree: the decision tree previously computed.\n",
    "\n",
    "    Returns:\n",
    "    - The predicted class label for the example.\n",
    "    \"\"\"\n",
    "\n",
    "    question = list(tree.keys())[0] # get the question at the current node of the tree\n",
    "    feature_name, comparison_operator, value = question.split(\" \") # for example feature name is 'stem-width', comparison operator is '<=' and value is '8.85'\n",
    "\n",
    "    # ask the question and determine which branch to follow\n",
    "    \n",
    "    # if feature is continuous, compare the feature value to the split value\n",
    "    if comparison_operator == \"<=\":  \n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0] # yes answer\n",
    "        else:\n",
    "            answer = tree[question][1] # no answer\n",
    "    \n",
    "    # if the feature is categorical, compare the feature value to the split value\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "    # if the answer is not a dictionary, then it is a leaf node, and we return the predicted class label\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part: if the answer is a dictionary, continue traversing the tree \n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict_example(example, residual_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df, tree):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the accuracy of a decision tree classifier on a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df: the input dataset \n",
    "    - tree: the decision tree previously computed\n",
    "\n",
    "    Returns:\n",
    "    - The accuracy of the decision tree on the given dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # we have to apply the prediction function to each row of the dataset\n",
    "    # The 'args' parameter is used to pass additional arguments to the function (i.e., the decision tree).\n",
    "    # 'axis=1' ensures that the function is applied row-wise.\n",
    "    df[\"classification\"] = df.apply(predict_example, args = (tree,), axis = 1) # this variable will contain the classfification of our examples\n",
    "\n",
    "    # new column to check if the classification is correct. It contains a boolean value (True/False) indicating whether the predicted class matches the true class.\n",
    "    df[\"classification_correct\"] = df[\"classification\"] == df[\"label\"]\n",
    "    \n",
    "    accuracy = df[\"classification_correct\"].mean() # mean of the 'classification_correct' column, which gives the proportion of correct classifications.\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one_loss(y_true, y_pred):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the zero-one loss between true labels and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: a list or array-like object containing the true class labels.\n",
    "    - y_pred: a list or array-like object containing the predicted class labels.\n",
    "\n",
    "    Returns:\n",
    "    - The zero-one loss, which represents the proportion of incorrect predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "\n",
    "    # element-wise comparison between true labels and predicted labels\n",
    "    # np.sum counts the number of False values (which indicate incorrect predictions)\n",
    "    \n",
    "    incorrect_predictions = np.sum(y_true != y_pred)\n",
    "    \n",
    "    loss = incorrect_predictions / len(y_true) #  proportion of incorrect predictions relative to the total number of samples\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_df, test_df = split_train_test(df, test_size = 0.2)\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    - min_samples: minimum number of samples required to split an internal node.\n",
    "    - max_depth: maximum depth of the tree.\n",
    "    - criterion: the criterion used to choose the best split ('scaled_entropy', 'gini', 'bhatt_coeff').\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tree = decision_tree(train_df, min_samples = 500, max_depth = 5, criterion='scaled_entropy')\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = test_df.iloc[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tree.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['stem-width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['stem-width'] <= 8.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = list(tree.keys())[0]\n",
    "tree[question][1] # no answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_example(example, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = [predict_example(train_df.loc[i], tree) for i in train_df.index]\n",
    "\n",
    "y_pred_test = [predict_example(test_df.loc[i], tree) for i in test_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_loss(y_true_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_loss(y_true_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(test_df, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(train_df, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['edible', 'poisoned']\n",
    "features = df.columns\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_true_test, y_pred_test),yticklabels=classes,\n",
    "            xticklabels=classes,annot=True,cmap='summer', fmt='g')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[38308]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[49293]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.iloc[:, :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.iloc[:, :-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {\"max_depth\": [], \"min_samples\": [], \"accuracy_train\": [], \"accuracy_test\": []}\n",
    "\n",
    "for max_depth in range(3, 7):\n",
    "    for min_samples in range(500, 3000, 500):\n",
    "\n",
    "        tree = decision_tree(train_df, max_depth=max_depth, min_samples=min_samples, criterion = 'scaled_entropy')\n",
    "\n",
    "        y_true_train = train_df['label'].values\n",
    "        y_true_test = test_df['label'].values\n",
    "\n",
    "        y_pred_train = [predict_example(train_df.loc[i], tree) for i in train_df.index]\n",
    "        y_pred_test = [predict_example(test_df.loc[i], tree) for i in test_df.index]\n",
    "\n",
    "\n",
    "        accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "        accuracy_test = accuracy_score(y_true_test, y_pred_test)\n",
    "\n",
    "        grid_search[\"max_depth\"].append(max_depth)\n",
    "        grid_search[\"min_samples\"].append(min_samples)\n",
    "        grid_search[\"accuracy_train\"].append(accuracy_train)\n",
    "        grid_search[\"accuracy_test\"].append(accuracy_test)\n",
    "        \n",
    "    print(f\"Progress: Iteration max_depth={max_depth}/6\")\n",
    "\n",
    "grid_search_df = pd.DataFrame(grid_search)\n",
    "grid_search_df.sort_values(\"accuracy_test\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "for max_depth in range(1, 25):\n",
    "    \n",
    "    tree = decision_tree(train_df, min_samples=500, max_depth=max_depth, criterion = 'scaled_entropy')\n",
    "\n",
    "    y_true_train = train_df['label'].values\n",
    "\n",
    "    \n",
    "    y_true_test = test_df['label'].values\n",
    "\n",
    "    y_pred_train = [predict_example(train_df.loc[i], tree) for i in train_df.index]\n",
    "    y_pred_test = [predict_example(test_df.loc[i], tree) for i in test_df.index]\n",
    "\n",
    "\n",
    "    accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_true_test, y_pred_test)\n",
    "\n",
    "    train_accuracies.append(accuracy_train)\n",
    "    test_accuracies.append(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(train_accuracies, label= \"train accuracy\")\n",
    "plt.plot(test_accuracies, label=\"test accuracy\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.xticks(range(0, 26, 5))\n",
    "plt.xlabel(\"max_depth\", size = 15)\n",
    "plt.ylabel(\"accuracy\", size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "train_df, test_df = split_train_test(df, test_size=0.2)\n",
    "tree = decision_tree(train_df, min_samples = 20, max_depth=5)\n",
    "accuracy = compute_accuracy(test_df, tree)\n",
    "\n",
    "pprint(tree)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_indices(df, k):\n",
    "    fold_size = len(df) // k\n",
    "    indices = np.arange(len(df))\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        test_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = np.concatenate([indices[:i * fold_size], indices[(i + 1) * fold_size:]])\n",
    "        folds.append((train_indices, test_indices))\n",
    "    return folds\n",
    "\n",
    "k = 5\n",
    "\n",
    "fold_indices = kfold_indices(df, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 1\n",
    "\n",
    "for train_indices, test_indices in fold_indices:\n",
    "    print(f'Fold:{cnt}, Train set: {len(train_indices)}, Test set:{len(test_indices)}')\n",
    "    cnt += 1\n",
    "    print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for train_indices, test_indices in fold_indices:\n",
    "    train_df = df.iloc[train_indices]\n",
    "    test_df = df.iloc[test_indices]\n",
    "\n",
    "    tree_cv = decision_tree(train_df, min_samples=500, max_depth=5, criterion='scaled_entropy')\n",
    "\n",
    "    y_true_test = test_df['label'].values\n",
    "    y_pred_test = [predict_example(test_df.loc[i], tree_cv) for i in test_df.index]\n",
    "\n",
    "    fold_score = accuracy_score(y_true_test, y_pred_test)\n",
    "\n",
    "    scores.append(fold_score)\n",
    "\n",
    "mean_accuracy = np.mean(scores)\n",
    "print(\"K-Fold Cross-Validation Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
